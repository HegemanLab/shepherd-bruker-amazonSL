# QE Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply prepend
# them with $. For strings the variable must be within quotes (ie, "$STR_VAR"),
# for numbers and booleans they should be plain (ie, $INT_VAR, $BOOL_VAR)


##################################################################################################
#
#    General Telegraf Set-Up
#
##################################################################################################


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"


# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "µs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = false
  ## Run telegraf in quiet mode (error log messages only).
  quiet = false
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false



##################################################################################################
#
#    Plugin Set-Up:  
#       Setting up how to input logs and output data.  
#
##################################################################################################



###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Configuration for sending metrics to InfluxDB
[[outputs.influxdb]]
  #urls = ["http://10.1.10.200:8086"] # required
  urls = ["http://10.1.101.200:8086/"] 
  ## The target database for metrics; will be created as needed.
  ## For UDP url endpoint database needs to be configured on server side.
  database = "bruker"


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# Stream and parse log file(s).
[[inputs.logparser]]
    ## Log files to parse.
    ## These accept standard unix glob matching rules, but with the addition of
       files = ["C://Program Files//Bruker Daltonik//HyStar//Logfiles//HyStarLogOutput.txt"]


    ## Read files that currently exist from the beginning. Files that are created
    ## while telegraf is running (and that match the "files" globs) will always
    ## be read from the beginning.
       from_beginning = true


    ## Parse logstash-style "grok" patterns:
       [inputs.logparser.grok]
       ## This is a list of patterns to check the given log file(s) for.
       ## Note that adding patterns here increases processing time. The most
       ## efficient configuration is to have one pattern per logparser.

          patterns = ['ErrorLB: \[%{NUMBER:time:ts-epoch}\, \{\"message":\"%{GREEDYDATA:Message}\"\}\]']
		

       ## Name of the outputted measurement name.
          measurement = "BrukerErrorLogs"

# Stream and parse log file(s).
[[inputs.logparser]]
    ## Log files to parse.
    ## These accept standard unix glob matching rules, but with the addition of
       files = ["C://Program Files//Bruker Daltonik//HyStar//Logfiles//HyStarLogOutput.txt"]


    ## Read files that currently exist from the beginning. Files that are created
    ## while telegraf is running (and that match the "files" globs) will always
    ## be read from the beginning.
       from_beginning = true


    ## Parse logstash-style "grok" patterns:
       [inputs.logparser.grok]
       ## This is a list of patterns to check the given log file(s) for.
       ## Note that adding patterns here increases processing time. The most
       ## efficient configuration is to have one pattern per logparser.

          patterns = ['LogBook: \[%{NUMBER:time:ts-epoch}\, \{\"message":\"%{GREEDYDATA:Message}\"\}\]']
		

       ## Name of the outputted measurement name.
          measurement = "BrukerLogs"

# Stream and parse log file(s).
[[inputs.logparser]]
    ## Log files to parse.
    ## These accept standard unix glob matching rules, but with the addition of
       files = ["C://Program Files//Bruker Daltonik//HyStar//Logfiles//HyStarLogOutput.txt"]


    ## Read files that currently exist from the beginning. Files that are created
    ## while telegraf is running (and that match the "files" globs) will always
    ## be read from the beginning.
       from_beginning = true


    ## Parse logstash-style "grok" patterns:
       [inputs.logparser.grok]
       ## This is a list of patterns to check the given log file(s) for.
       ## Note that adding patterns here increases processing time. The most
       ## efficient configuration is to have one pattern per logparser.

          patterns = ['LBTable: \[%{NUMBER:time:ts-epoch}\, \{\"SampleTableName":\"%{GREEDYDATA:SampleTableName}\", \"LineInTable\"\:\"%{GREEDYDATA:LineInTable}\", \"SampleID\"\:\"%{GREEDYDATA:SampleID}\", \"VialPosition\"\:\"%{GREEDYDATA:VialPosition}\", \"LCMethod\"\:\"%{GREEDYDATA:LCMethod}\", \"AutoSamplerPart\"\:\"%{GREEDYDATA:AutoSamplerPart}\", \"MSPart\"\:\"%{GREEDYDATA:MSPart}\", \"StoreResultsIn\":\"%{GREEDYDATA:StoreResultsIn}\", \"MethodInResults\":\"%{DATA:MethodInResults}\",']
		

       ## Name of the outputted measurement name.
          measurement = "BrukerSampleTable"